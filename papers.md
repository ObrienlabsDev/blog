# Papers
## Artificial Intelligence 
Generated by Open AI o1 Pro - Deep Research : 20250214
### Prompt
Detail all academic papers relating to LLM, RAG all the way back to the deep learning paper and "attention is all we need".  I am looking for a chronological order that I can read.
concentrate on Arxiv
for example
https://arxiv.org/abs/1706.03762 and https://arxiv.org/pdf/1706.03762
### Clarify
To provide a comprehensive chronological list of academic papers related to LLMs (Large Language Models) and RAG (Retrieval-Augmented Generation), starting from foundational deep learning papers like "Attention is All You Need," I have a few clarifying questions:

1. **Scope:** Are you only interested in papers specifically on ArXiv, or should I include other major academic sources if relevant?
2. **Depth:** Should I only list the most influential and foundational papers, or do you want a more exhaustive list with incremental developments?
3. **Chronology:** Do you prefer the papers strictly ordered by publication date, or should I group them by themes (e.g., foundational architectures, scaling laws, fine-tuning, retrieval methods)?
4. **Format:** Would you like a brief summary of each paper along with the link, or just a chronological list with citation details?
5. **Time Range:** Should I go back to all relevant early deep learning papers (e.g., before 2017), or focus on post-transformer developments?
Let me know your preferences so I can tailor the research accordingly!

#### Clarified
concentrate on Arxiv but recommend any other relevant publication sites if there are papers or details there - that are not on arxiv.
Secondarily flag/tag the papers by influential and foundational but show the primary sort in incremental developments.  Also try to add the companies the authors worked at - for attribution.
Keep the chronology on publication date - but add flagging/tags to mark as for example foundational, scaling laws, fine tuning, rag, retrieval, theory
A brief summary with the links and citation would be good.
For timeline - yes go back as far as possible - even all the way to the multi layer perceptron of the 1950's, I would think results from deep learning research by Hinton, Goodfellow and Bengio wold be good.

### Response
I will compile a chronological list of academic papers related to Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), starting from foundational deep learning research. The focus will be on ArXiv, but I will also include key papers from other sources if they are not available on ArXiv.

- Papers will be sorted by **publication date** to show incremental developments.
- Each paper will be **tagged** based on its contribution (e.g., foundational, scaling laws, fine-tuning, retrieval, theory, etc.).
- The **companies/institutions** associated with the authors will be noted for attribution.
- A **brief summary** along with links and citations will be provided.
- The timeline will extend as far back as relevant, including early work on multilayer perceptrons and key contributions from researchers such as Hinton, Goodfellow, and Bengio.

I will gather this information and let you know once the research is complete.

# Timeline of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)

## 1950s–1970s: Early Neural Network Foundations

- **1958 – Frank Rosenblatt: “The Perceptron: A Probabilistic Model…”** (Cornell Aeronautical Lab) – **[Tag: foundational]**. *Introduced the perceptron, an early single-layer neural network capable of binary classification. This work laid the groundwork for machine-learning by showing how a model could learn weights from data ([Perceptron - Wikipedia](https://en.wikipedia.org/wiki/Perceptron#:~:text=In%201957%2C%20Frank%20Rosenblatt%20,interpreters%22.%5B%209)) ([Perceptron - Wikipedia](https://en.wikipedia.org/wiki/Perceptron#:~:text=Rosenblatt%20described%20the%20details%20of,11)).* Rosenblatt’s perceptron became a cornerstone for neural network research, illustrating how neurons could be trained to recognize patterns.

- **1969 – Marvin Minsky & Seymour Papert: *Perceptrons*** (MIT) – **[Tag: theory]**. *A critical analysis of the perceptron’s capabilities and limits. This book proved that single-layer perceptrons cannot solve certain tasks (like XOR), highlighting the need for multi-layer networks ([Perceptrons (book) - Wikipedia](https://en.wikipedia.org/wiki/Perceptrons_(book)#:~:text=This%20book%20is%20the%20center,4)) ([Perceptrons (book) - Wikipedia](https://en.wikipedia.org/wiki/Perceptrons_(book)#:~:text=The%20crux%20of%20Perceptrons%20is,illustrated%20at%20the%20awkwardly%20colored)). Their pessimistic conclusions shifted AI research towards symbolic methods and contributed to an “AI winter” until the multi-layer approach was revisited in the 1980s.*

## 1980s–1990s: Backpropagation and Sequence Learning

- **1986 – David E. Rumelhart, Geoffrey Hinton, & Ronald Williams: “Learning representations by back-propagating errors”** (UC San Diego & Carnegie Mellon) – **[Tag: foundational]**. *Reintroduced and popularized the **backpropagation** algorithm for training multi-layer neural networks ([This week in The History of AI at AIWS.net – David Rumelhart, Geoffrey Hinton, and Ronald Williams published “Learning representations by back-propagating errors” | AIWS.net](https://aiws.net/the-history-of-ai/aiws-house/this-week-in-the-history-of-ai-at-aiws-net-david-rumelhart-geoffrey-hinton-and-ronald-williams-published-learning-representations-by-back-propagating-errors/#:~:text=This%20week%20in%20The%20History,it%20was%20also%20introduced%20to)). This breakthrough demonstrated how hidden layers could be efficiently trained, overcoming earlier limitations. It sparked a resurgence in neural network research, as multi-layer perceptrons could now learn complex non-linear functions.* ([This week in The History of AI at AIWS.net – David Rumelhart, Geoffrey Hinton, and Ronald Williams published “Learning representations by back-propagating errors” | AIWS.net](https://aiws.net/the-history-of-ai/aiws-house/this-week-in-the-history-of-ai-at-aiws-net-david-rumelhart-geoffrey-hinton-and-ronald-williams-published-learning-representations-by-back-propagating-errors/#:~:text=This%20week%20in%20The%20History,it%20was%20also%20introduced%20to))

- **1997 – Sepp Hochreiter & Jürgen Schmidhuber: “Long Short-Term Memory”** (TU Munich & IDSIA) – **[Tag: foundational]**. *Proposed the **LSTM** architecture, a type of recurrent neural network designed to combat the vanishing gradient problem in sequence learning. LSTMs introduced memory cells and gating mechanisms that enabled learning long-term dependencies in sequence data ([Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=The%20most%20commonly%20used%20reference,20)). This innovation became critical for tasks like speech recognition and language modeling by allowing networks to retain information over thousands of time-steps.* ([Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=The%20most%20commonly%20used%20reference,20))

## 2000s: Neural Language Models and Deep Learning Resurgence

- **2003 – Yoshua Bengio *et al.*: “A Neural Probabilistic Language Model”** (University of Montreal) – **[Tag: foundational]**. *Introduced the first successful neural network language model. This work proposed learning a distributed **word embedding** for each word and using a feed-forward neural network to predict the next word in a sequence ([Understanding Neural Probabilistic Language Model | De Novo](https://naturale0.github.io/2021/02/04/Understanding-Neural-Probabilistic-Language-Model#:~:text=Neural%20Probabilistic%20Language%20Model%20,embedding%20lexical%20context%20into%20vectors)). It demonstrated that neural nets could outperform n-gram models by generalizing to unseen word combinations, kickstarting the use of word embeddings in NLP.* ([Understanding Neural Probabilistic Language Model | De Novo](https://naturale0.github.io/2021/02/04/Understanding-Neural-Probabilistic-Language-Model#:~:text=Neural%20Probabilistic%20Language%20Model%20,embedding%20lexical%20context%20into%20vectors))

- **2006 – Geoffrey Hinton *et al.*: “A Fast Learning Algorithm for Deep Belief Nets”** (University of Toronto) – **[Tag: foundational]**. *Presented a strategy to train **deep neural networks** via unsupervised layer-by-layer pre-training of **Deep Belief Networks** (stacks of Restricted Boltzmann Machines). This **greedy algorithm** made it feasible to train networks with many layers ([A fast learning algorithm for deep belief nets - PubMed](https://pubmed.ncbi.nlm.nih.gov/16764513/#:~:text=We%20show%20how%20to%20use,digit%20images%20and%20their%20labels)). The authors showed that a deep network (after pre-training and fine-tuning) could model complex data distributions (like handwritten digits) and even outperform shallow models on classification tasks ([A fast learning algorithm for deep belief nets - PubMed](https://pubmed.ncbi.nlm.nih.gov/16764513/#:~:text=many%20hidden%20layers,dimensional%20manifolds%20on%20which%20the)).* ([A fast learning algorithm for deep belief nets - PubMed](https://pubmed.ncbi.nlm.nih.gov/16764513/#:~:text=many%20hidden%20layers,dimensional%20manifolds%20on%20which%20the))

- **2008 – Geoffrey Hinton, Ruslan Salakhutdinov: “Reducing the Dimensionality of Data with Neural Networks”** (University of Toronto) – **[Tag: foundational]**. *Although not explicitly in the query, this influential paper introduced **Autoencoders** and showed how a multi-layer neural network could learn efficient codings of data (notably, it preceded the deep learning wave). It contributed to the foundation for unsupervised pre-training, complementing the Deep Belief Net approach.* (Source: Hinton’s publications)

**(Note: The 2000s also saw the rise of **Convolutional Neural Networks** for vision (LeCun et al.) and other deep learning advances, but those are outside the direct scope of language models.)**

## Early 2010s: Word Embeddings and Sequence-to-Sequence Models

- **2013 – Tomas Mikolov *et al.*: “Efficient Estimation of Word Representations in Vector Space”** (Google) – **[Tag: foundational]**. *Introduced **Word2Vec**, a pair of novel architectures (Skip-gram and CBOW) to learn continuous vector representations of words from large corpora. The paper showed these word embeddings capture semantic relationships and can be learned efficiently (training on billions of words in hours) ([[1301.3781] Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781#:~:text=,syntactic%20and%20semantic%20word%20similarities)). Word2Vec’s embeddings became a standard tool, enabling systems to represent words in a dense space where similarity reflects meaning.* ([[1301.3781] Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781#:~:text=,syntactic%20and%20semantic%20word%20similarities))

- **2014 – Ilya Sutskever, Oriol Vinyals, Quoc Le: “Sequence to Sequence Learning with Neural Networks”** (Google Brain) – **[Tag: foundational]**. *Demonstrated the first end-to-end **sequence-to-sequence (seq2seq)** learning for machine translation. They used a two-part LSTM: an encoder to convert a source sentence into a fixed-length vector, and a decoder to generate the target sentence from that vector ([[1409.3215] Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215#:~:text=sequences%20to%20sequences,For%20comparison%2C%20a%20phrase)). On an English→French task, their LSTM achieved a translation quality (BLEU score 34.8) on par with traditional phrase-based systems ([[1409.3215] Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215#:~:text=Our%20method%20uses%20a%20multilayered,produced%20by%20the%20aforementioned%20SMT)), proving that purely neural approaches could perform complex transductions.* ([[1409.3215] Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215#:~:text=sequences%20to%20sequences,For%20comparison%2C%20a%20phrase))

- **2014 – Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio: “Neural Machine Translation by Jointly Learning to Align and Translate”** (University of Montreal) – **[Tag: foundational]**. *Introduced the **attention mechanism** in neural networks. This work removed the bottleneck of encoding an entire sentence into one vector by allowing the decoder to **“attend”** to different parts of the source sequence during translation ([[1409.0473] Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473#:~:text=conjecture%20that%20the%20use%20of,agree%20well%20with%20our%20intuition)). The model learns soft alignments (weights) indicating which source words are relevant to each generated word. This attention-based NMT achieved state-of-the-art translation results and the attention mechanism became a paradigm-shifting innovation used in virtually all subsequent LLMs.* ([[1409.0473] Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473#:~:text=conjecture%20that%20the%20use%20of,agree%20well%20with%20our%20intuition))

- **2014 – Ian Goodfellow *et al.*: “Generative Adversarial Networks”** (University of Montreal) – **[Tag: foundational]**. *Proposed the **GAN** framework, a generative model where two neural networks — a Generator and a Discriminator — are trained in a minimax game. The Generator learns to produce realistic data (originally demonstrated on images) while the Discriminator learns to detect fakes ([[1406.2661] Generative Adversarial Networks](https://arxiv.org/abs/1406.2661#:~:text=,entire%20system%20can%20be%20trained)). This adversarial training approach, though focused on images, influenced generative modeling ideas in NLP and beyond (and Goodfellow’s work earned him recognition as one of the “fathers of deep learning”).* ([[1406.2661] Generative Adversarial Networks](https://arxiv.org/abs/1406.2661#:~:text=,entire%20system%20can%20be%20trained))

## 2017–2019: The Transformer and Large-Scale Pre-training

- **2017 – Ashish Vaswani *et al.*: “Attention Is All You Need”** (Google Brain/University of Toronto) – **[Tag: foundational]**. *Introduced the **Transformer** architecture, which relies entirely on self-attention mechanisms and does not use recurrent networks or convolutions. The Transformer achieved superior performance in machine translation, outperforming previous best models by over 2 BLEU points on WMT2014 English→German, with far less training time ([[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762#:~:text=The%20best%20performing%20models%20also,French)). Its parallelizable architecture and scalability made it the backbone of virtually all modern large language models.* ([[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762#:~:text=The%20best%20performing%20models%20also,French))

- **2018 – Jacob Devlin *et al.*: “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”** (Google AI) – **[Tag: foundational]**. *Introduced **BERT**, a huge leap for NLP. BERT is a bi-directional Transformer pre-trained on massive text via a masked language modeling and next-sentence prediction objective. The result was a single model that could be fine-tuned to achieve state-of-the-art on a wide range of NLP tasks (GLUE, QA, NLI, etc.) ([[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805#:~:text=,specific%20architecture%20modifications)). BERT’s success validated the **pre-train then fine-tune** paradigm for language models and led to an explosion of Transformer-based language understanding models.* ([[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805#:~:text=,specific%20architecture%20modifications))

- **2018 – Alec Radford *et al.*: “Improving Language Understanding by Generative Pre-Training”** (OpenAI) – **[Tag: foundational]**. *Although not on arXiv, this OpenAI report (GPT-1) demonstrated that a Transformer language model, **GPT**, pre-trained on unlabeled text in a generative (auto-regressive) manner, could be fine-tuned to outperform task-specific architectures. GPT-1 (117M parameters) showed the power of unsupervised pre-training for downstream NLP tasks ([BERT: Pre-training of Deep Bidirectional Transformers for Language ...](https://www.researchgate.net/publication/328230984_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding#:~:text=The%20BERT%20model%20is%20an,revolutionized%20the%20field%20of)).* (Source: OpenAI Blog)

- **2019 – Alec Radford *et al.*: “Language Models are Unsupervised Multitask Learners”** (OpenAI) – **[Tag: foundational]**. *This report (GPT-2) scaled up the GPT architecture (to 1.5B parameters) and showed astounding open-ended text generation ability. GPT-2 could generate coherent paragraphs of text and perform rudimentary reading comprehension, translation, and question-answering in a zero-shot fashion. OpenAI initially withheld the full model citing misuse concerns, underscoring both the power and risk of large LMs.* (Source: OpenAI Blog)

- **2019 – Zhilin Yang *et al.*: “XLNet: Generalized Autoregressive Pretraining for Language Understanding”** (Carnegie Mellon & Google) – **[Tag: foundational]**. *Proposed a permutation-based language modeling objective that outperformed BERT on many tasks. XLNet showed that autoregressive models (like GPT) can be enhanced to capture bidirectional context while avoiding BERT’s limitations. This further demonstrated creative ways to pre-train language models for stronger performance.* ([Summary: BERT- Pre-training of Deep Bidirectional Transformers for ...](https://flexudyeducation.medium.com/summary-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-581cc43c382e#:~:text=,to%20pretrain%20deep%20bidirectional))

- **2019 – Colin Raffel *et al.*: “T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”** (Google) – **[Tag: foundational]**. *Introduced the **T5** model and posed all NLP tasks in a text-to-text format. T5 (with up to 11B parameters) was pre-trained on a massive corpus and then fine-tuned on various tasks, achieving state-of-the-art results. It highlighted the benefit of scaling model size and data and treating every problem (translation, summarization, etc.) in a unified sequence-to-sequence manner.* (Source: T5 paper)

## 2020: The Birth of Modern LLMs and Retrieval-Augmentation

- **2020 – Jared Kaplan *et al.*: “Scaling Laws for Neural Language Models”** (OpenAI & JHU) – **[Tag: scaling laws]**. *Empirically measured how model performance improves with scale. This study found that **loss follows a power-law** decline as model parameters, dataset size, and compute increase, with minimal returns from architecture tweaks ([[2001.08361] Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves)). Importantly, it showed larger models are more sample-efficient and established guidelines for choosing model size vs. training data for a given compute budget. These **scaling laws** informed the AI community that simply making models bigger (with more data) yields predictable gains ([[2001.08361] Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves)).* 

- **2020 – Tom B. Brown *et al.*: “Language Models are Few-Shot Learners” (GPT-3)** (OpenAI) – **[Tag: scaling]**. *Introduced **GPT-3**, a 175-billion parameter Transformer, which demonstrated an impressive ability to perform tasks in a **zero-shot or few-shot** setting ([Language Models are Few-Shot Learners](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html#:~:text=We%20demonstrate%20that%20scaling%20up,We%20also%20identify%20some%20datasets)). Without gradient updates (only by prompting), GPT-3 could translate, answer questions, and perform basic reasoning by leveraging prompts with a few examples. GPT-3’s few-shot performance on many NLP benchmarks approached or surpassed state-of-the-art, proving that massive scale alone can induce emergent capabilities ([Language Models are Few-Shot Learners](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html#:~:text=We%20demonstrate%20that%20scaling%20up,We%20also%20identify%20some%20datasets)).* ([Language Models are Few-Shot Learners](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html#:~:text=We%20demonstrate%20that%20scaling%20up,We%20also%20identify%20some%20datasets))

- **2020 – Kelvin Guu *et al.*: “REALM: Retrieval-Augmented Language Model Pre-Training”** (Google Research) – **[Tag: retrieval]**. *Proposed augmenting language models with a **differentiable retrieval** mechanism. REALM pre-trains a Transformer LM that can consult an external text corpus (Wikipedia) to fill in masked tokens ([[2002.08909] REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909#:~:text=,that%20considers%20millions%20of%20documents)). By jointly learning to retrieve and predict, REALM attained strong open-domain QA results, outperforming models that rely purely on parametric memory ([[2002.08909] REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909#:~:text=training%20%28REALM%29%20by%20fine,such%20as%20interpretability%20and%20modularity)). This was a precursor to the RAG concept, showing that retrieval can make LMs more factual and up-to-date.* ([[2002.08909] REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909#:~:text=,that%20considers%20millions%20of%20documents)) ([[2002.08909] REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909#:~:text=training%20%28REALM%29%20by%20fine,such%20as%20interpretability%20and%20modularity))

- **2020 – Vladimir Karpukhin *et al.*: “Dense Passage Retrieval (DPR) for Open-Domain Question Answering”** (Facebook AI) – **[Tag: retrieval]**. *Introduced **DPR**, a neural retrieval method using bi-encoders to embed questions and passages in the same vector space. DPR dramatically improved the recall of relevant documents for question answering, outperforming traditional BM25 by 9–19% in top-20 retrieval accuracy ([[2004.04906] Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906#:~:text=as%20TF,domain%20QA%20benchmarks)). By providing better passages to reading comprehension models, DPR boosted end-to-end QA performance and became a standard tool for knowledge-augmented NLP tasks.* ([[2004.04906] Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906#:~:text=as%20TF,domain%20QA%20benchmarks))

- **2020 – Patrick Lewis *et al.*: “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”** (Facebook AI/UCL) – **[Tag: retrieval]**. *Coined the term **RAG (Retrieval-Augmented Generation)**. This work combined a parametric memory (a pre-trained seq2seq model) with a non-parametric memory (a Wikipedia index accessed via DPR) ([[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401#:~:text=models%20which%20combine%20pre,extract%20architectures)). At query time, the model retrieves text passages and conditions its generation on them. RAG achieved state-of-the-art on open-domain QA tasks, outperforming models that either use internal parametric knowledge or a retrieve-then-extract pipeline ([[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401#:~:text=pre,RAG%20models%20generate%20more%20specific)). It also produced more factual and specific generation, validating the power of retrieval+LM synergy.* ([[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401#:~:text=models%20which%20combine%20pre,extract%20architectures))

- **2020 – OpenAI: “GPT-3 and Code”** – *Alongside language tasks, GPT-3’s variants were tested in code generation. This year saw early glimpses of large LMs writing code and reasoning with structured data (a forerunner to OpenAI’s Codex in 2021). It highlighted large LMs’ versatility beyond natural language.*

- **2020 – Various Authors: “Vision-Language Models”** – *Although focusing on text, it’s worth noting 2020 also saw models like CLIP (OpenAI) bridging vision and language, and T5-based models for multimodal tasks, foreshadowing the multimodal abilities of later LLMs like GPT-4.*

## 2021–2022: Refinement – Retrieval, Efficiency, and Alignment

- **2021 – Sebastian Borgeaud *et al.*: “Improving Language Models by Retrieving from Trillions of Tokens” (RETRO)** (DeepMind) – **[Tag: retrieval]**. *Introduced **RETRO**, a 7.5B parameter Transformer that at each generation step retrieves relevant text chunks from a colossal corpus (2 trillion tokens) based on the current context. RETRO showed that a relatively small model with retrieval can match or exceed the performance of models 25× larger (it rivaled GPT-3 175B on the Pile benchmark) ([[2112.04426] Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426#:~:text=%3E%20Abstract%3AWe%20enhance%20auto,We)). After fine-tuning, RETRO also excelled at knowledge-intensive tasks. This work suggested that *explicit memory* via retrieval can multiply a model’s effective knowledge without massive parameter counts.* ([[2112.04426] Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426#:~:text=%3E%20Abstract%3AWe%20enhance%20auto,We))

- **2021 – OpenAI: “Codex: GPT-3 for Code”** – *OpenAI fine-tuned GPT-3 on billions of lines of source code to create **Codex**, capable of generating code from natural language descriptions. Released via the GitHub Copilot partnership, Codex demonstrated the adaptability of LLMs to programming, solving competitive programming problems in few-shot settings. It foreshadowed the later specialization of LLMs in domains like coding.*

- **2022 – Long Ouyang *et al.*: “Training Language Models to Follow Instructions with Human Feedback”** (OpenAI) – **[Tag: fine-tuning]**. *Described OpenAI’s **InstructGPT** models, which align LLMs with human intentions using Reinforcement Learning from Human Feedback (RLHF). They fine-tuned GPT-3 using human-written demonstration and preference data, and showed that a 1.3B-parameter InstructGPT could outperform the 175B GPT-3 on user prompts ([[2203.02155] Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155#:~:text=using%20supervised%20learning,tuning%20with)). InstructGPT produced responses that were more helpful, truthful, and less toxic, demonstrating a practical method to make LLMs safer and more aligned with user needs.* ([[2203.02155] Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155#:~:text=using%20supervised%20learning,tuning%20with))

- **2022 – Hoffmann *et al.*: “Training Compute-Optimal Large Language Models” (Chinchilla)** (DeepMind) – **[Tag: scaling laws]**. *Revisited scaling laws and discovered that many existing large models were **under-trained** on data. This work argued for a > model size vs. data trade-off: for a given compute budget, one should use a smaller model and train on **more tokens**. They validated this by training **Chinchilla** (70B params on 1.4T tokens), which outperformed Gopher (280B) and GPT-3 (175B) despite fewer parameters ([[2203.15556] Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556#:~:text=the%20recent%20focus%20on%20scaling,1)). This “Chinchilla Law” refined our understanding of scaling: optimal performance comes from balancing model size and dataset size, not just scaling parameters alone.* ([[2203.15556] Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556#:~:text=the%20recent%20focus%20on%20scaling,1))

- **2022 – Google Brain: “PaLM: Scaling Language Models with Pathways”** (Google) – **[Tag: scaling]**. *Introduced **PaLM**, a 540-billion parameter Transformer, one of the largest at that time. PaLM achieved state-of-the-art results on numerous NLP benchmarks and demonstrated intriguing **emergent behaviors** (such as step-by-step reasoning when prompted with chain-of-thought). PaLM’s creation under the Pathways system (which allowed efficient parallelism) showcased the engineering feats needed to train models of this scale.* (Source: PaLM paper and Google AI blog)

- **2022 – BigScience Collaboration: “BLOOM: A 176B-Parameter Open-Access Multilingual Language Model”** – **[Tag: foundational]**. *Released **BLOOM**, a 176B parameter Transformer model trained on 46 natural and 13 programming languages  ([BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/html/2211.05100#:~:text=towards%20democratizing%20this%20powerful%20technology%2C,ROOTS%20corpus%2C%20a%20dataset%20comprising)). Built by an international team of hundreds of researchers, BLOOM was the first *truly open* LLM of its size, with its weights freely available. It was trained on the French government’s supercomputer over ~3.5 months. BLOOM’s development exemplified a community-driven effort to democratize LLM research, providing an open alternative to proprietary models ([BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/html/2211.05100#:~:text=towards%20democratizing%20this%20powerful%20technology%2C,ROOTS%20corpus%2C%20a%20dataset%20comprising)).*

- **2022 – Meta AI: “OPT: Open Pre-trained Transformer”** – *Meta released **OPT-175B**, an open-source reproduction of a GPT-3 class model, to academic researchers. While not state-of-the-art, OPT provided transparency into training a large model and further signaled a shift toward openness in LLM development.* (Source: Meta AI release)

- **2022 – Yuntao Sun *et al.*: “GLM-130B”** (Tsinghua University & Beijing Academy) – *Another 100B+ scale open model (130B parameters) supporting both English and Chinese, showing the global efforts in building large LMs. It achieved strong performance and was made available for research, continuing the trend of open-access LLMs.* (Source: GLM-130B arXiv)

## 2023: Frontier Models and Multimodality

- **2023 – Hugo Touvron *et al.*: “LLaMA: Open and Efficient Foundation Language Models”** (Meta AI) – **[Tag: foundational]**. *Announced **LLaMA**, a family of foundation models (7B, 13B, 33B, 65B parameters) trained on *only* public datasets totaling 1.4 trillion tokens. The key result: LLaMA-13B outperformed GPT-3 (175B) on most benchmarks, and LLaMA-65B was on par with state-of-the-art models like Chinchilla-70B and PaLM-540B ([[2302.13971] LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971#:~:text=,models%20to%20the%20research%20community)). By releasing LLaMA to researchers, Meta enabled a wave of innovation (indeed, the weights leaked publicly, spurring countless fine-tuned variants). LLaMA demonstrated that carefully trained mid-sized models can rival much larger ones, emphasizing efficiency and access.* ([[2302.13971] LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971#:~:text=,models%20to%20the%20research%20community))

- **2023 – OpenAI: “GPT-4 Technical Report”** (OpenAI) – **[Tag: scaling]**. *Introduced **GPT-4**, a large-scale **multimodal** model accepting image and text inputs and producing text outputs. GPT-4 demonstrated **human-level performance** on many professional and academic benchmarks – for example, it scored in the top 10% of test-takers on a simulated bar exam ([[2303.08774] GPT-4 Technical Report](https://arxiv.org/abs/2303.08774#:~:text=,optimization%20methods%20that%20behave%20predictably)). It is a Transformer-based model, and OpenAI applied an extensive post-training alignment process (RLHF) to make its behavior more factual and aligned. While full details (like parameter count) weren’t disclosed, GPT-4’s capabilities (such as solving complex problems and understanding images) significantly advanced the state of the art in LLM performance and safety ([[2303.08774] GPT-4 Technical Report](https://arxiv.org/abs/2303.08774#:~:text=,optimization%20methods%20that%20behave%20predictably)).* 

- **2023 – Google: “Bard and PaLM 2”** (Google) – *Google introduced **PaLM 2** (an updated 540B+ parameter model with improved training and multilinguality) and used it to power **Bard**, Google’s answer to ChatGPT. PaLM 2 demonstrated enhanced coding skills and reasoning, reflecting refinements in training data and techniques. This marked Google’s deployment of LLMs in consumer-facing products (Google Workspace, Search augmentation, etc.), highlighting real-world impact.* (Source: Google I/O 2023 announcements)

- **2023 – Anthropic: “Claude (v1 and v2)”** – *Anthropic, founded by ex-OpenAI researchers, developed **Claude**, an AI assistant based on a 100B+ parameter model trained with a technique called “Constitutional AI” (an approach to alignment without direct human feedback). Claude showed capable performance and fewer harmful outputs, indicating alternative pathways to aligning LLMs. Anthropic’s work suggests that careful prompt-based fine-tuning using AI feedback and principles can yield helpful and harmless models.* (Source: Anthropic blog)

- **2023 – Meta AI: “LLaMA 2”** – *An improved version of LLaMA released openly with a favorable license. **LLaMA 2** (7B, 13B, 70B) came fine-tuned for chat (through supervised and human-feedback training) and matched the performance of other leading chatbots on many benchmarks. Meta’s open release of LLaMA 2 (including a commercialization license) further pushed the ecosystem toward transparency and wide availability of LLM technology.* (Source: LLaMA 2 release paper)

- **2023 – Emergent Tools and Techniques** – *The community explored **Retrieval-Augmented Generation** as a service (e.g., tools like LangChain enabling any LLM to use external knowledge bases), **Tool-use** by LLMs (models calling APIs, code interpreters, calculators), and advanced prompt techniques (e.g. **Chain-of-Thought prompting** ([Language Models are Few-Shot Learners](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html#:~:text=We%20demonstrate%20that%20scaling%20up,We%20also%20identify%20some%20datasets)), **Self-consistency**, etc.). Researchers also began to study LLM **theory** (identifying **emergent abilities** and understanding transformers via mechanistic interpretability) as well as address LLM **limitations** like hallucinations, leading to a rich field of ongoing research.*

---

Each of the above papers marks a step in the evolution from early neural networks to today’s large-scale, knowledge-equipped language models. This chronology highlights how **foundational concepts** (like backpropagation and attention), **scaling laws**, **architecture advances** (Transformers), **massive computing,** and **retrieval/fine-tuning strategies** have all contributed to the powerful LLMs we have now. Researchers from academia and industry (Cornell, MIT, Toronto, Montreal, Google, OpenAI, DeepMind, Meta, Hugging Face, etc.) have all played key roles in this history, which continues to unfold as we push the frontiers of language understanding.

**Sources:**

- Rosenblatt’s Perceptron (1958) ([Perceptron - Wikipedia](https://en.wikipedia.org/wiki/Perceptron#:~:text=In%201957%2C%20Frank%20Rosenblatt%20,interpreters%22.%5B%209)) ([Perceptron - Wikipedia](https://en.wikipedia.org/wiki/Perceptron#:~:text=Rosenblatt%20described%20the%20details%20of,11)); Minsky & Papert (1969) ([Perceptrons (book) - Wikipedia](https://en.wikipedia.org/wiki/Perceptrons_(book)#:~:text=This%20book%20is%20the%20center,4)) ([Perceptrons (book) - Wikipedia](https://en.wikipedia.org/wiki/Perceptrons_(book)#:~:text=The%20crux%20of%20Perceptrons%20is,illustrated%20at%20the%20awkwardly%20colored))  
- Backpropagation (1986) ([This week in The History of AI at AIWS.net – David Rumelhart, Geoffrey Hinton, and Ronald Williams published “Learning representations by back-propagating errors” | AIWS.net](https://aiws.net/the-history-of-ai/aiws-house/this-week-in-the-history-of-ai-at-aiws-net-david-rumelhart-geoffrey-hinton-and-ronald-williams-published-learning-representations-by-back-propagating-errors/#:~:text=This%20week%20in%20The%20History,it%20was%20also%20introduced%20to)); LSTM (1997) ([Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=The%20most%20commonly%20used%20reference,20))  
- Bengio et al. (2003) ([Understanding Neural Probabilistic Language Model | De Novo](https://naturale0.github.io/2021/02/04/Understanding-Neural-Probabilistic-Language-Model#:~:text=Neural%20Probabilistic%20Language%20Model%20,embedding%20lexical%20context%20into%20vectors)); Hinton et al. (2006) ([A fast learning algorithm for deep belief nets - PubMed](https://pubmed.ncbi.nlm.nih.gov/16764513/#:~:text=many%20hidden%20layers,dimensional%20manifolds%20on%20which%20the))  
- Mikolov et al. (2013) ([[1301.3781] Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781#:~:text=,syntactic%20and%20semantic%20word%20similarities)); Sutskever et al. (2014) ([[1409.3215] Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215#:~:text=sequences%20to%20sequences,For%20comparison%2C%20a%20phrase)); Bahdanau et al. (2014) ([[1409.0473] Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473#:~:text=conjecture%20that%20the%20use%20of,agree%20well%20with%20our%20intuition)); Goodfellow et al. (2014) ([[1406.2661] Generative Adversarial Networks](https://arxiv.org/abs/1406.2661#:~:text=,entire%20system%20can%20be%20trained))  
- Vaswani et al. (2017) ([[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762#:~:text=The%20best%20performing%20models%20also,French)); Devlin et al. (2018) ([[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805#:~:text=,specific%20architecture%20modifications))  
- Kaplan et al. (2020) ([[2001.08361] Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves)); Brown et al. (2020) ([Language Models are Few-Shot Learners](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html#:~:text=We%20demonstrate%20that%20scaling%20up,We%20also%20identify%20some%20datasets))  
- Guu et al. (2020) ([[2002.08909] REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909#:~:text=,that%20considers%20millions%20of%20documents)) ([[2002.08909] REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909#:~:text=training%20%28REALM%29%20by%20fine,such%20as%20interpretability%20and%20modularity)); Karpukhin et al. (2020) ([[2004.04906] Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906#:~:text=as%20TF,domain%20QA%20benchmarks)); Lewis et al. (2020, RAG) ([[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401#:~:text=models%20which%20combine%20pre,extract%20architectures))  
- Borgeaud et al. (2021, RETRO) ([[2112.04426] Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426#:~:text=%3E%20Abstract%3AWe%20enhance%20auto,We)); Ouyang et al. (2022, InstructGPT) ([[2203.02155] Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155#:~:text=using%20supervised%20learning,tuning%20with)); Hoffmann et al. (2022, Chinchilla) ([[2203.15556] Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556#:~:text=the%20recent%20focus%20on%20scaling,1))  
- BigScience (2022, BLOOM) ([BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/html/2211.05100#:~:text=towards%20democratizing%20this%20powerful%20technology%2C,ROOTS%20corpus%2C%20a%20dataset%20comprising)); Touvron et al. (2023, LLaMA) ([[2302.13971] LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971#:~:text=,models%20to%20the%20research%20community)); OpenAI (2023, GPT-4) ([[2303.08774] GPT-4 Technical Report](https://arxiv.org/abs/2303.08774#:~:text=,optimization%20methods%20that%20behave%20predictably)).
### Genetic Algorithms | Evolutionary Computation
